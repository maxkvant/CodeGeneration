{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.1.4 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from pandas==2.1.4) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from pandas==2.1.4) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from pandas==2.1.4) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from pandas==2.1.4) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: loguru==0.7.2 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (0.7.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy==1.26.4 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from scikit-learn==1.3.2) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from scikit-learn==1.3.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/maxim/Projects/ml/CodeGeneration/venv/lib/python3.12/site-packages (from scikit-learn==1.3.2) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 requests-2.32.3 urllib3-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas==2.1.4\n",
    "%pip install loguru==0.7.2\n",
    "%pip install numpy==1.26.4\n",
    "%pip install scikit-learn==1.3.2\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from graph import Step\n",
    "from language_modeling import OpenAiLlamaApi, LlamaModel, PromptGenerator\n",
    "from code_generation import ValidationCodeGenerator, MainCodeGenerator\n",
    "from orchestrator import Orchestrator\n",
    "from utils import get_dataset_info\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_STEP_SCRIPT = \"\"\"\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def step_40(Segments_normalized, Dec_levels):\n",
    "    Features = []\n",
    "    for segment in Segments_normalized:\n",
    "        coeffs = pywt.wavedec(segment, 'db4', level=Dec_levels)\n",
    "        features = [coefficient.mean() for coefficient in coeffs]\n",
    "        Features.append(features)\n",
    "    return StandardScaler().fit_transform(Features)\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLE_VALIDATION_SCRIPT = \"\"\"\n",
    "import pandas as pd\n",
    "from step_10 import step_10\n",
    "from step_20 import step_20\n",
    "from step_30 import step_30\n",
    "from step_40 import step_40\n",
    "\n",
    "def validate_step():\n",
    "    csv_path = '/path/to/your/csv/file.csv'\n",
    "    raw_data = step_10(csv_path)\n",
    "    Segments = step_20(raw_data, SizeSegment=512)\n",
    "    Segments_normalized = step_30(Segments)\n",
    "    Features = step_40(Segments_normalized, Dec_levels=5)\n",
    "    print(Features)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate_step()\n",
    "\"\"\"\n",
    "\n",
    "steps = [\n",
    "    Step(\n",
    "        step_id=\"10\",\n",
    "        description=\"Import raw data from CSV and segment it\",\n",
    "        dependencies=[],\n",
    "        input_vars=[\"csv_path\", \"SizeSegment\"],\n",
    "        output_vars=[\"Segments\"],\n",
    "        additional_info=\"Use pandas to read the CSV and create segments of size SizeSegment.\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"20\",\n",
    "        description=\"Normalize the segmented data using MinMaxScaler\",\n",
    "        dependencies=[\"10\"],\n",
    "        input_vars=[\"Segments\"],\n",
    "        output_vars=[\"Segments_normalized\"],\n",
    "        additional_info=\"Segments is a list of 1D numpy arrays. Each segment should be normalized independently.\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"30\",\n",
    "        description=\"Extract features using wavelet decomposition\",\n",
    "        dependencies=[\"20\"],\n",
    "        input_vars=[\"Segments_normalized\", \"Dec_levels\"],\n",
    "        output_vars=[\"Features\"],\n",
    "        additional_info=\"Use pywavelets (pywt) library with 'db3' wavelet and specified Dec_levels.\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"40\",\n",
    "        description=\"Apply PCA for dimension reduction\",\n",
    "        dependencies=[\"30\"],\n",
    "        input_vars=[\"Features\", \"NC_pca\"],\n",
    "        output_vars=[\"PCA_Features\", \"pca\"],\n",
    "        additional_info=\"Use sklearn's PCA. Return both the transformed features and the PCA object.\"\n",
    "    ),\n",
    "    Step(step_id=\"50\",\n",
    "        description=\"Train model, evaluate, and calculate metrics\",\n",
    "        dependencies=[\"40\"],\n",
    "        input_vars=[\"PCA_Features\", \"kernel\", \"nu\", \"gamma\"],\n",
    "        output_vars=[\"FittedClassifier\", \"Prec_learn\", \"Prec_test\"],\n",
    "        additional_info=\"\"\"\n",
    "        1. Create labels: np.ones for learning data.\n",
    "        2. Split data into train and test sets (80% train, 20% test).\n",
    "        3. Create and fit a One-Class SVM classifier using sklearn.\n",
    "        4. Predict labels for training data.\n",
    "        5. Calculate error rate for training data.\n",
    "        6. Predict labels for test data (assume all test data as anomaly, i.e., -1).\n",
    "        7. Calculate error rate for test data.\n",
    "        8. Calculate precision as 1 - error_rate for both training and test.\n",
    "        Return the fitted classifier and both precision values.\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = str(Path('learning-file_2.csv').resolve())\n",
    "raw_data = pd.read_csv(csv_path)\n",
    "dataset_info = get_dataset_info(raw_data)\n",
    "# Assume raw_data is a pandas DataFrame with 'timestamp' and 'signal' columns\n",
    "signal_data = raw_data['signal'].values\n",
    "\n",
    "# Adjust based on data size\n",
    "SizeSegment = min(512, len(signal_data) // 100)\n",
    "gamma = 'scale'  # Let sklearn choose an appropriate scale\n",
    "nu = 0.1  # This might need domain knowledge to set appropriately\n",
    "kernel = \"rbf\"  # This is often a good default\n",
    "\n",
    "# PCA\n",
    "# We'll use the signal data for PCA parameter calculation\n",
    "pca = PCA().fit(signal_data.reshape(-1, 1))\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "NC_pca = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "Dec_levels = int(np.log2(SizeSegment)) - 3  # Adjust based on segment size\n",
    "\n",
    "parameters = {\n",
    "    'csv_path': f\"'{csv_path}'\",\n",
    "    \"SizeSegment\": f\"{SizeSegment}\",\n",
    "    \"gamma\": f\"'{gamma}'\",\n",
    "    \"nu\": f\"{nu}\",\n",
    "    \"kernel\" : f\"'{kernel}'\",\n",
    "    \"NC_pca\": f\"{NC_pca}\",\n",
    "    \"Dec_levels\": f\"{Dec_levels}\",\n",
    "}\n",
    "\n",
    "with open('env.json', 'r') as f:\n",
    "    credentials_dict = json.load(f)\n",
    "\n",
    "API_URL = \"https://openrouter.ai/api/v1\"\n",
    "API_KEY = credentials_dict[\"OPENROUTER_API_KEY\"]\n",
    "MODEL_TAG = \"meta-llama/llama-3-70b-instruct\"\n",
    "llama_api = OpenAiLlamaApi(API_URL, API_KEY, MODEL_TAG)\n",
    "model = LlamaModel(llama_api)\n",
    "prompt_generator = PromptGenerator(EXAMPLE_STEP_SCRIPT, dataset_info)\n",
    "validation_code_genrator = ValidationCodeGenerator()\n",
    "main_code_generator = MainCodeGenerator()\n",
    "\n",
    "orchestrator = Orchestrator(\n",
    "    model,\n",
    "    prompt_generator,\n",
    "    validation_code_genrator,\n",
    "    main_code_generator,\n",
    "    'out'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:06.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-d4Np0neV6cBQrMBNiImnFvSMjucw', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137781, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function `step_10`:\\n```\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\ndef step_10(csv_path, SizeSegment):\\n    data = pd.read_csv(csv_path)\\n    signal_values = data['signal'].values\\n    num_segments = int(np.ceil(len(signal_values) / SizeSegment))\\n    Segments = [signal_values[i*SizeSegment:(i+1)*SizeSegment] for i in range(num_segments)]\\n    Segments_normalized = [MinMaxScaler().fit_transform(segment.reshape(-1, 1)).ravel() for segment in Segments]\\n    return Segments_normalized\\n```\"}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 712, 'completion_tokens': 136, 'total_tokens': 848}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated step source for step 10, filename: step_10.py\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "def step_10(csv_path, SizeSegment):\n",
      "    data = pd.read_csv(csv_path)\n",
      "    signal_values = data['signal'].values\n",
      "    num_segments = int(np.ceil(len(signal_values) / SizeSegment))\n",
      "    Segments = [signal_values[i*SizeSegment:(i+1)*SizeSegment] for i in range(num_segments)]\n",
      "    Segments_normalized = [MinMaxScaler().fit_transform(segment.reshape(-1, 1)).ravel() for segment in Segments]\n",
      "    return Segments_normalized\n",
      "\n",
      "generated validation source for step 10, filename: validate_step_10.py\n",
      "import pandas as pd\n",
      "from step_10 import step_10\n",
      "\n",
      "\n",
      "csv_path = '/Users/maxim/Projects/ml/CodeGeneration/src/learning-file_2.csv'\n",
      "SizeSegment = 307\n",
      "gamma = 'scale'\n",
      "nu = 0.1\n",
      "kernel = 'rbf'\n",
      "NC_pca = 1\n",
      "Dec_levels = 5\n",
      "\n",
      "def validate_step():\n",
      "    Segments = step_10(csv_path, SizeSegment)\n",
      "    print(Segments)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\n",
      "\n",
      "running validate_step_10.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:15.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-mDRIAwtM42cWXDWMEjvmoAx4LDX6', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137787, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function 'step_20' that normalizes the segmented data using MinMaxScaler:\\n```\\nimport numpy as np\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\ndef step_20(Segments):\\n    Segments_normalized = []\\n    for segment in Segments:\\n        scaler = MinMaxScaler()\\n        normalized_segment = scaler.fit_transform(segment.reshape(-1, 1)).flatten()\\n        Segments_normalized.append(normalized_segment)\\n    return Segments_normalized\\n```\"}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 715, 'completion_tokens': 98, 'total_tokens': 813}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated step source for step 20, filename: step_20.py\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "def step_20(Segments):\n",
      "    Segments_normalized = []\n",
      "    for segment in Segments:\n",
      "        scaler = MinMaxScaler()\n",
      "        normalized_segment = scaler.fit_transform(segment.reshape(-1, 1)).flatten()\n",
      "        Segments_normalized.append(normalized_segment)\n",
      "    return Segments_normalized\n",
      "\n",
      "generated validation source for step 20, filename: validate_step_20.py\n",
      "import pandas as pd\n",
      "from step_10 import step_10\n",
      "from step_20 import step_20\n",
      "\n",
      "\n",
      "csv_path = '/Users/maxim/Projects/ml/CodeGeneration/src/learning-file_2.csv'\n",
      "SizeSegment = 307\n",
      "gamma = 'scale'\n",
      "nu = 0.1\n",
      "kernel = 'rbf'\n",
      "NC_pca = 1\n",
      "Dec_levels = 5\n",
      "\n",
      "def validate_step():\n",
      "    Segments = step_10(csv_path, SizeSegment)\n",
      "    Segments_normalized = step_20(Segments)\n",
      "    print(Segments_normalized)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\n",
      "\n",
      "running validate_step_20.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:19.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-dXakTtEzI1OQDtHD1xfQVA12YHrD', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137796, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function `step_30`:\\n```\\nimport pywt\\nimport numpy as np\\n\\ndef step_30(Segments_normalized, Dec_levels):\\n    Features = []\\n    for segment in Segments_normalized:\\n        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\\n        features = [np.mean(coefficient) for coefficient in coeffs]\\n        Features.append(features)\\n    return Features\\n```\"}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 717, 'completion_tokens': 90, 'total_tokens': 807}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated step source for step 30, filename: step_30.py\n",
      "\n",
      "import pywt\n",
      "import numpy as np\n",
      "\n",
      "def step_30(Segments_normalized, Dec_levels):\n",
      "    Features = []\n",
      "    for segment in Segments_normalized:\n",
      "        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\n",
      "        features = [np.mean(coefficient) for coefficient in coeffs]\n",
      "        Features.append(features)\n",
      "    return Features\n",
      "\n",
      "generated validation source for step 30, filename: validate_step_30.py\n",
      "import pandas as pd\n",
      "from step_10 import step_10\n",
      "from step_20 import step_20\n",
      "from step_30 import step_30\n",
      "\n",
      "\n",
      "csv_path = '/Users/maxim/Projects/ml/CodeGeneration/src/learning-file_2.csv'\n",
      "SizeSegment = 307\n",
      "gamma = 'scale'\n",
      "nu = 0.1\n",
      "kernel = 'rbf'\n",
      "NC_pca = 1\n",
      "Dec_levels = 5\n",
      "\n",
      "def validate_step():\n",
      "    Segments = step_10(csv_path, SizeSegment)\n",
      "    Segments_normalized = step_20(Segments)\n",
      "    Features = step_30(Segments_normalized, Dec_levels)\n",
      "    print(Features)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\n",
      "\n",
      "running validate_step_30.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:21.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-VVZJezTUrVWtgXKFdTsazpXTQRTB', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137800, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"```\\nimport numpy as np\\nimport赞pywavelets as pywt\\n\\ndef step_30(Segments_normalized, Dec_levels):\\n    Features = []\\n    for segment in Segments_normalized:\\n        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\\n        features = [np.mean(coefficient) for coefficient in coeffs]\\n        Features.append(features)\\n    return Features\\n```\"}, 'finish_reason': 'stop', 'logprobs': None}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 203, 'completion_tokens': 83, 'total_tokens': 286}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 30, filename: step_30.py\n",
      "import numpy as np\n",
      "import赞pywavelets as pywt\n",
      "\n",
      "def step_30(Segments_normalized, Dec_levels):\n",
      "    Features = []\n",
      "    for segment in Segments_normalized:\n",
      "        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\n",
      "        features = [np.mean(coefficient) for coefficient in coeffs]\n",
      "        Features.append(features)\n",
      "    return Features\n",
      "\n",
      "running validate_step_30.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:29.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-p3gzTf3RBtaTExIujatSxODNQIYh', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137802, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the corrected code snippet:\\n\\n```\\nimport numpy as np\\nimport pywt\\n\\ndef step_30(Segments_normalized, Dec_levels):\\n    Features = []\\n    for segment in Segments_normalized:\\n        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\\n        features = [np.mean(coefficient) for coefficient in coeffs]\\n        Features.append(features)\\n    return Features\\n```\"}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 207, 'completion_tokens': 86, 'total_tokens': 293}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 30, filename: step_30.py\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pywt\n",
      "\n",
      "def step_30(Segments_normalized, Dec_levels):\n",
      "    Features = []\n",
      "    for segment in Segments_normalized:\n",
      "        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\n",
      "        features = [np.mean(coefficient) for coefficient in coeffs]\n",
      "        Features.append(features)\n",
      "    return Features\n",
      "\n",
      "running validate_step_30.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:35.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-Zl1i9xIxeLRMlo1cuYi1Qp5XwnS1', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137810, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"```\\nimport numpy as np\\nimport pywavelets as pywt\\n\\ndef step_30(Segments_normalized, Dec_levels):\\n    Features = []\\n    for segment in Segments_normalized:\\n        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\\n        features = [np.mean(coefficient) for coefficient in coeffs]\\n        Features.append(features)\\n    return Features\\n```\"}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 202, 'completion_tokens': 82, 'total_tokens': 284}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 30, filename: step_30.py\n",
      "import numpy as np\n",
      "import pywavelets as pywt\n",
      "\n",
      "def step_30(Segments_normalized, Dec_levels):\n",
      "    Features = []\n",
      "    for segment in Segments_normalized:\n",
      "        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\n",
      "        features = [np.mean(coefficient) for coefficient in coeffs]\n",
      "        Features.append(features)\n",
      "    return Features\n",
      "\n",
      "running validate_step_30.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:38.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-0FAkHbEZohSiXwVxZYg0CvVl4fIt', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137817, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"import numpy as np\\nimport pywt\\n\\ndef step_30(Segments_normalized, Dec_levels):\\n    Features = []\\n    for segment in Segments_normalized:\\n        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\\n        features = [np.mean(coefficient) for coefficient in coeffs]\\n        Features.append(features)\\n    return Features\"}, 'finish_reason': 'stop', 'logprobs': None}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 212, 'completion_tokens': 74, 'total_tokens': 286}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 30, filename: step_30.py\n",
      "import numpy as np\n",
      "import pywt\n",
      "\n",
      "def step_30(Segments_normalized, Dec_levels):\n",
      "    Features = []\n",
      "    for segment in Segments_normalized:\n",
      "        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\n",
      "        features = [np.mean(coefficient) for coefficient in coeffs]\n",
      "        Features.append(features)\n",
      "    return Features\n",
      "\n",
      "running validate_step_30.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:40.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-cxTAttzzHH96D9gGDhll1M3qPqfB', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137819, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'pip install PyWavelets\\n\\nimport numpy as np\\nimport pywt'}, 'finish_reason': 'eos', 'logprobs': None}], 'usage': {'prompt_tokens': 203, 'completion_tokens': 15, 'total_tokens': 218}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 30, filename: step_30.py\n",
      "pip install PyWavelets\n",
      "\n",
      "import numpy as np\n",
      "import pywt\n",
      "\n",
      "running validate_step_30.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:46.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-Pj8nm4USITspLMeUFzCcKxAZbeWu', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137821, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the function definition:\\n\\n```\\nimport pandas as pd\\nfrom sklearn.decomposition import PCA\\n\\ndef step_40(Features, NC_pca):\\n    pca = PCA(n_components=NC_pca)\\n    PCA_Features = pca.fit_transform(Features)\\n    return PCA_Features, pca\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 713, 'completion_tokens': 62, 'total_tokens': 775}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated step source for step 40, filename: step_40.py\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "def step_40(Features, NC_pca):\n",
      "    pca = PCA(n_components=NC_pca)\n",
      "    PCA_Features = pca.fit_transform(Features)\n",
      "    return PCA_Features, pca\n",
      "\n",
      "generated validation source for step 40, filename: validate_step_40.py\n",
      "import pandas as pd\n",
      "from step_30 import step_30\n",
      "from step_10 import step_10\n",
      "from step_20 import step_20\n",
      "from step_40 import step_40\n",
      "\n",
      "\n",
      "csv_path = '/Users/maxim/Projects/ml/CodeGeneration/src/learning-file_2.csv'\n",
      "SizeSegment = 307\n",
      "gamma = 'scale'\n",
      "nu = 0.1\n",
      "kernel = 'rbf'\n",
      "NC_pca = 1\n",
      "Dec_levels = 5\n",
      "\n",
      "def validate_step():\n",
      "    Features = step_30(Segments_normalized, Dec_levels)\n",
      "    Segments = step_10(csv_path, SizeSegment)\n",
      "    Segments_normalized = step_20(Segments)\n",
      "    PCA_Features, pca = step_40(Features, NC_pca)\n",
      "    print(PCA_Features, pca)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\n",
      "\n",
      "running validate_step_40.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:48.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-TgQkdkI6m7BDOvE35aiK4ljFvUGI', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137827, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code snippet:\\n\\n```\\nimport pandas as pd\\nfrom sklearn.decomposition import PCA\\n\\ndef step_40(Features, NC_pca):\\n    pca = PCA(n_components=NC_pca)\\n    PCA_Features = pca.fit_transform(Features)\\n    return PCA_Features, pca\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 180, 'completion_tokens': 63, 'total_tokens': 243}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 40, filename: step_40.py\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "def step_40(Features, NC_pca):\n",
      "    pca = PCA(n_components=NC_pca)\n",
      "    PCA_Features = pca.fit_transform(Features)\n",
      "    return PCA_Features, pca\n",
      "\n",
      "running validate_step_40.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:51.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-yWhQlJ9BiVN5oMpnpHABj8OS5f6n', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137829, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```\\nimport pandas as pd\\nfrom sklearn.decomposition import PCA\\n\\ndef step_40(Features, NC_pca):\\n    pca = PCA(n_components=NC_pca)\\n    PCA_Features = pca.fit_transform(Features)\\n    return PCA_Features, pca\\n```'}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 179, 'completion_tokens': 57, 'total_tokens': 236}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 40, filename: step_40.py\n",
      "import pandas as pd\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "def step_40(Features, NC_pca):\n",
      "    pca = PCA(n_components=NC_pca)\n",
      "    PCA_Features = pca.fit_transform(Features)\n",
      "    return PCA_Features, pca\n",
      "\n",
      "running validate_step_40.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:54.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-WgEuOHRtclkjm2bQXGaM504vWqZ2', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137832, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code snippet:\\n\\n```\\nimport pandas as pd\\nfrom sklearn.decomposition import PCA\\n\\ndef step_40(Features, NC_pca):\\n    pca = PCA(n_components=NC_pca)\\n    PCA_Features = pca.fit_transform(Features)\\n    return PCA_Features, pca\\n```'}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 179, 'completion_tokens': 64, 'total_tokens': 243}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 40, filename: step_40.py\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "def step_40(Features, NC_pca):\n",
      "    pca = PCA(n_components=NC_pca)\n",
      "    PCA_Features = pca.fit_transform(Features)\n",
      "    return PCA_Features, pca\n",
      "\n",
      "running validate_step_40.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:58.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-H4tS8U7JrfyBo33LR3whFBHADusM', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137835, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code snippet:\\n\\n```\\nimport pandas as pd\\nfrom sklearn.decomposition import PCA\\n\\ndef step_40(Features, NC_pca):\\n    pca = PCA(n_components=NC_pca)\\n    PCA_Features = pca.fit_transform(Features)\\n    return PCA_Features, pca\\n```'}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 179, 'completion_tokens': 64, 'total_tokens': 243}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 40, filename: step_40.py\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "def step_40(Features, NC_pca):\n",
      "    pca = PCA(n_components=NC_pca)\n",
      "    PCA_Features = pca.fit_transform(Features)\n",
      "    return PCA_Features, pca\n",
      "\n",
      "running validate_step_40.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:23:59.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-yiitSJsujsRGRV9YZfUnKQvCkvvK', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137838, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the fixed code:\\n\\n```\\nimport pandas as pd\\nfrom sklearn.decomposition import PCA\\n\\ndef step_40(Features, NC_pca):\\n    pca = PCA(n_components=NC_pca)\\n    PCA_Features = pca.fit_transform(Features)\\n    return PCA_Features, pca\\n```'}, 'finish_reason': 'eos', 'logprobs': None}], 'usage': {'prompt_tokens': 180, 'completion_tokens': 63, 'total_tokens': 243}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 40, filename: step_40.py\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "def step_40(Features, NC_pca):\n",
      "    pca = PCA(n_components=NC_pca)\n",
      "    PCA_Features = pca.fit_transform(Features)\n",
      "    return PCA_Features, pca\n",
      "\n",
      "running validate_step_40.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:24:16.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-Q6ENCFfoUqtVmC627mzeuTFmmqhF', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137840, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the Python function definition for `step_50`:\\n```\\nimport numpy as np\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.model_selection import train_test_split\\n\\ndef step_50(PC_Features, kernel, nu, gamma):\\n    labels = np.ones(len(PC_Features))\\n    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\\n    \\n    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\\n    clf.fit(X_train)\\n    \\n    y_pred_train = clf.predict(X_train)\\n    error_rate_train = np.mean(y_pred_train!= 1)\\n    Prec_learn = 1 - error_rate_train\\n    \\n    y_pred_test = clf.predict(X_test)\\n    error_rate_test = np.mean(y_pred_test!= -1)\\n    Prec_test = 1 - error_rate_test\\n    \\n    return clf, Prec_learn, Prec_test\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 843, 'completion_tokens': 197, 'total_tokens': 1040}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated step source for step 50, filename: step_50.py\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.svm import OneClassSVM\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def step_50(PC_Features, kernel, nu, gamma):\n",
      "    labels = np.ones(len(PC_Features))\n",
      "    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\n",
      "    \n",
      "    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\n",
      "    clf.fit(X_train)\n",
      "    \n",
      "    y_pred_train = clf.predict(X_train)\n",
      "    error_rate_train = np.mean(y_pred_train!= 1)\n",
      "    Prec_learn = 1 - error_rate_train\n",
      "    \n",
      "    y_pred_test = clf.predict(X_test)\n",
      "    error_rate_test = np.mean(y_pred_test!= -1)\n",
      "    Prec_test = 1 - error_rate_test\n",
      "    \n",
      "    return clf, Prec_learn, Prec_test\n",
      "\n",
      "generated validation source for step 50, filename: validate_step_50.py\n",
      "import pandas as pd\n",
      "from step_10 import step_10\n",
      "from step_30 import step_30\n",
      "from step_20 import step_20\n",
      "from step_40 import step_40\n",
      "from step_50 import step_50\n",
      "\n",
      "\n",
      "csv_path = '/Users/maxim/Projects/ml/CodeGeneration/src/learning-file_2.csv'\n",
      "SizeSegment = 307\n",
      "gamma = 'scale'\n",
      "nu = 0.1\n",
      "kernel = 'rbf'\n",
      "NC_pca = 1\n",
      "Dec_levels = 5\n",
      "\n",
      "def validate_step():\n",
      "    Segments = step_10(csv_path, SizeSegment)\n",
      "    Features = step_30(Segments_normalized, Dec_levels)\n",
      "    Segments_normalized = step_20(Segments)\n",
      "    PCA_Features, pca = step_40(Features, NC_pca)\n",
      "    FittedClassifier, Prec_learn, Prec_test = step_50(PCA_Features, kernel, nu, gamma)\n",
      "    print(FittedClassifier, Prec_learn, Prec_test)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\n",
      "\n",
      "running validate_step_50.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:24:18.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-kjFQgRbluSIGZnm0oNbZNZGs6MCk', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137857, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```\\nimport numpy as np\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.model_selection import train_test_split\\n\\ndef step_50(PC_Features, kernel, nu, gamma):\\n    labels = np.ones(len(PC_Features))\\n    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\\n    \\n    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\\n    clf.fit(X_train)\\n    \\n    y_pred_train = clf.predict(X_train)\\n    error_rate_train = np.mean(y_pred_train != 1)\\n    Prec_learn = 1 - error_rate_train\\n    \\n    y_pred_test = clf.predict(X_test)\\n    error_rate_test = np.mean(y_pred_test != -1)\\n    Prec_test = 1 - error_rate_test\\n    \\n    return clf, Prec_learn, Prec_test\\n```'}, 'finish_reason': 'eos', 'logprobs': None}], 'usage': {'prompt_tokens': 308, 'completion_tokens': 185, 'total_tokens': 493}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 50, filename: step_50.py\n",
      "import numpy as np\n",
      "from sklearn.svm import OneClassSVM\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def step_50(PC_Features, kernel, nu, gamma):\n",
      "    labels = np.ones(len(PC_Features))\n",
      "    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\n",
      "    \n",
      "    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\n",
      "    clf.fit(X_train)\n",
      "    \n",
      "    y_pred_train = clf.predict(X_train)\n",
      "    error_rate_train = np.mean(y_pred_train != 1)\n",
      "    Prec_learn = 1 - error_rate_train\n",
      "    \n",
      "    y_pred_test = clf.predict(X_test)\n",
      "    error_rate_test = np.mean(y_pred_test != -1)\n",
      "    Prec_test = 1 - error_rate_test\n",
      "    \n",
      "    return clf, Prec_learn, Prec_test\n",
      "\n",
      "running validate_step_50.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:24:25.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-K1rGekK6BdhMGrMZoSH2ecyJTu7F', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137859, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the fixed code snippet:\\n\\n```\\nimport numpy as np\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.model_selection import train_test_split\\n\\ndef step_50(PC_Features, kernel, nu, gamma):\\n    labels = np.ones(len(PC_Features))\\n    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\\n    \\n    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\\n    clf.fit(X_train)\\n    \\n    y_pred_train = clf.predict(X_train)\\n    error_rate_train = np.mean(y_pred_train != 1)\\n    Prec_learn = 1 - error_rate_train\\n    \\n    y_pred_test = clf.predict(X_test)\\n    error_rate_test = np.mean(y_pred_test != -1)\\n    Prec_test = 1 - error_rate_test\\n    \\n    return clf, Prec_learn, Prec_test\\n```'}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 307, 'completion_tokens': 192, 'total_tokens': 499}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 50, filename: step_50.py\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.svm import OneClassSVM\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def step_50(PC_Features, kernel, nu, gamma):\n",
      "    labels = np.ones(len(PC_Features))\n",
      "    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\n",
      "    \n",
      "    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\n",
      "    clf.fit(X_train)\n",
      "    \n",
      "    y_pred_train = clf.predict(X_train)\n",
      "    error_rate_train = np.mean(y_pred_train != 1)\n",
      "    Prec_learn = 1 - error_rate_train\n",
      "    \n",
      "    y_pred_test = clf.predict(X_test)\n",
      "    error_rate_test = np.mean(y_pred_test != -1)\n",
      "    Prec_test = 1 - error_rate_test\n",
      "    \n",
      "    return clf, Prec_learn, Prec_test\n",
      "\n",
      "running validate_step_50.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:24:28.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-Q6YApdvOA0SMGgU3VPYNTSvPmSRL', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137866, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code snippet:\\n\\n```\\nimport numpy as np\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.model_selection import train_test_split\\n\\ndef step_50(PC_Features, kernel, nu, gamma):\\n    labels = np.ones(len(PC_Features))\\n    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\\n    \\n    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\\n    clf.fit(X_train)\\n    \\n    y_pred_train = clf.predict(X_train)\\n    error_rate_train = np.mean(y_pred_train != 1)\\n    Prec_learn = 1 - error_rate_train\\n    \\n    y_pred_test = clf.predict(X_test)\\n    error_rate_test = np.mean(y_pred_test != 1)\\n    Prec_test = 1 - error_rate_test\\n    \\n    return clf, Prec_learn, Prec_test\\n```'}, 'finish_reason': 'eos', 'logprobs': None}], 'usage': {'prompt_tokens': 308, 'completion_tokens': 192, 'total_tokens': 500}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 50, filename: step_50.py\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.svm import OneClassSVM\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def step_50(PC_Features, kernel, nu, gamma):\n",
      "    labels = np.ones(len(PC_Features))\n",
      "    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\n",
      "    \n",
      "    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\n",
      "    clf.fit(X_train)\n",
      "    \n",
      "    y_pred_train = clf.predict(X_train)\n",
      "    error_rate_train = np.mean(y_pred_train != 1)\n",
      "    Prec_learn = 1 - error_rate_train\n",
      "    \n",
      "    y_pred_test = clf.predict(X_test)\n",
      "    error_rate_test = np.mean(y_pred_test != 1)\n",
      "    Prec_test = 1 - error_rate_test\n",
      "    \n",
      "    return clf, Prec_learn, Prec_test\n",
      "\n",
      "running validate_step_50.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:24:34.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-yexnLXoWr2iObcWQG0ZB30VAySIn', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137870, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code:\\n\\n```\\nimport numpy as np\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.model_selection import train_test_split\\n\\ndef step_50(PC_Features, kernel, nu, gamma):\\n    labels = np.ones(len(PC_Features))\\n    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\\n    \\n    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\\n    clf.fit(X_train)\\n    \\n    y_pred_train = clf.predict(X_train)\\n    error_rate_train = np.mean(y_pred_train != 1)\\n    Prec_learn = 1 - error_rate_train\\n    \\n    y_pred_test = clf.predict(X_test)\\n    error_rate_test = np.mean(y_pred_test != 1)\\n    Prec_test = 1 - error_rate_test\\n    \\n    return clf, Prec_learn, Prec_test\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 308, 'completion_tokens': 190, 'total_tokens': 498}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 50, filename: step_50.py\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.svm import OneClassSVM\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def step_50(PC_Features, kernel, nu, gamma):\n",
      "    labels = np.ones(len(PC_Features))\n",
      "    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\n",
      "    \n",
      "    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\n",
      "    clf.fit(X_train)\n",
      "    \n",
      "    y_pred_train = clf.predict(X_train)\n",
      "    error_rate_train = np.mean(y_pred_train != 1)\n",
      "    Prec_learn = 1 - error_rate_train\n",
      "    \n",
      "    y_pred_test = clf.predict(X_test)\n",
      "    error_rate_test = np.mean(y_pred_test != 1)\n",
      "    Prec_test = 1 - error_rate_test\n",
      "    \n",
      "    return clf, Prec_learn, Prec_test\n",
      "\n",
      "running validate_step_50.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-08 20:24:38.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mFull API response: {'id': 'gen-IHnA0nB3JRKBDZ6GnzynzSmLpUPa', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723137875, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```\\nimport numpy as np\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.model_selection import train_test_split\\n\\ndef step_50(PC_Features, kernel, nu, gamma):\\n    labels = np.ones(len(PC_Features))\\n    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\\n    \\n    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\\n    clf.fit(X_train)\\n    \\n    y_pred_train = clf.predict(X_train)\\n    error_rate_train = np.mean(y_pred_train != 1)\\n    Prec_learn = 1 - error_rate_train\\n    \\n    y_pred_test = clf.predict(X_test)\\n    error_rate_test = np.mean(y_pred_test != 1)\\n    Prec_test = 1 - error_rate_test\\n    \\n    return clf, Prec_learn, Prec_test\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 308, 'completion_tokens': 184, 'total_tokens': 492}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing step source for step 50, filename: step_50.py\n",
      "import numpy as np\n",
      "from sklearn.svm import OneClassSVM\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def step_50(PC_Features, kernel, nu, gamma):\n",
      "    labels = np.ones(len(PC_Features))\n",
      "    X_train, X_test = train_test_split(PC_Features, test_size=0.2, random_state=42)\n",
      "    \n",
      "    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\n",
      "    clf.fit(X_train)\n",
      "    \n",
      "    y_pred_train = clf.predict(X_train)\n",
      "    error_rate_train = np.mean(y_pred_train != 1)\n",
      "    Prec_learn = 1 - error_rate_train\n",
      "    \n",
      "    y_pred_test = clf.predict(X_test)\n",
      "    error_rate_test = np.mean(y_pred_test != 1)\n",
      "    Prec_test = 1 - error_rate_test\n",
      "    \n",
      "    return clf, Prec_learn, Prec_test\n",
      "\n",
      "running validate_step_50.py in /Users/maxim/Projects/ml/CodeGeneration/src/out\n",
      "exit_code = 1\n"
     ]
    }
   ],
   "source": [
    "orchestrator.run_steps(steps, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
